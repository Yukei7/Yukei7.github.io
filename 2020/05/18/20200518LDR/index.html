<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"always","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="主要包括主成分分析（PCA）和经典相关分析（CCA）的推导">
<meta property="og:type" content="article">
<meta property="og:title" content="Linear Dimensionality Reduction">
<meta property="og:url" content="http://yoursite.com/2020/05/18/20200518LDR/index.html">
<meta property="og:site_name" content="Yukei">
<meta property="og:description" content="主要包括主成分分析（PCA）和经典相关分析（CCA）的推导">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/05/18/20200518LDR/2020-05-18-01-38-49.png">
<meta property="og:image" content="http://yoursite.com/2020/05/18/20200518LDR/2020-05-17-05-31-45.png">
<meta property="og:image" content="http://yoursite.com/2020/05/18/20200518LDR/2020-05-17-05-36-49.png">
<meta property="article:published_time" content="2020-05-17T17:34:17.000Z">
<meta property="article:modified_time" content="2020-05-17T17:56:00.602Z">
<meta property="article:author" content="Yukei Yim">
<meta property="article:tag" content="Multivariate Statistics">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/05/18/20200518LDR/2020-05-18-01-38-49.png">

<link rel="canonical" href="http://yoursite.com/2020/05/18/20200518LDR/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Linear Dimensionality Reduction | Yukei</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-161580624-1"></script>
    <script>
      var host = window.location.hostname;
      if (host !== "localhost" || !true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-161580624-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yukei</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/18/20200518LDR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yukei Yim">
      <meta itemprop="description" content="学数学本是逆天而行">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yukei">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Linear Dimensionality Reduction
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-18 01:34:17 / 修改时间：01:56:00" itemprop="dateCreated datePublished" datetime="2020-05-18T01:34:17+08:00">2020-05-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/maths/" itemprop="url" rel="index">
                    <span itemprop="name">maths</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/05/18/20200518LDR/2020-05-18-01-38-49.png"> 主要包括主成分分析（PCA）和经典相关分析（CCA）的推导 <a id="more"></a></p>
<p>Outline:</p>
<ul>
<li>Introduction</li>
<li>Principal Component Analysis</li>
<li>Canonical Variate and Correlation Analysis</li>
</ul>
<h2 id="introduction">7.1 Introduction</h2>
<p><strong>Target:</strong> find a projection that project data onto a lower-dimensional subspace without losing important information.</p>
<ul>
<li>One possible way to accomplish this is through <strong>feature selection</strong>.</li>
<li>Another way is by creating a reduced set of transformations(linear or nonlinear) of the input variables, which is often referred to as <strong>feature extraction</strong>.</li>
</ul>
<p><strong>Principal Component Analysis(PCA)</strong> and <strong>Cononical Variate and Correlation Analysis(CVA/CCA)</strong> are linear methods that are most popular in use today. Both these mothods are <strong>eigenvalue-eigenvector problems</strong> and can be viewed as special cases of multivariate reduced-rank regression.</p>
<h2 id="principal-component-analysis">7.2 Principal Component Analysis</h2>
<p>PCA was introduced as technique for deriving a reduced set of <strong>orthogonal linear projections</strong> of a single collection of correlated variables <span class="math inline">\(X=(X_1,...,X_r)^T\)</span>, where the projections are oredered by decreasing variances. It's also a method of <strong>decorrelating</strong> <span class="math inline">\(X\)</span>.</p>
<p><strong>Note:</strong> Variance is a second-order property of a random variable and is an important measurement of the amount of information in that variable.</p>
<p>How to make use of the principal component scores (in decreasing order):</p>
<ul>
<li>The first few ones: revealing whether most of the data actually live on a linear subspace of <span class="math inline">\(\mathbb{R}^r\)</span> and identifying outliers, distributional peculiarities and clusters of points.</li>
<li>The last few ones: detect collinearity and outliers that pop up and alter the perceived dimensionality of the data (which has near-zero variance)</li>
</ul>
<h3 id="population-principal-components">7.2.1 Population Principal Components</h3>
<p>Assume that the ramdom <span class="math inline">\(r\)</span>-vector <span class="math inline">\(X=(X_1,...,X_r)^T\)</span> has mean <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\((r\times r)\)</span> covariance matrix <span class="math inline">\(\Sigma_{XX}\)</span>.</p>
<p>PCA: set of <span class="math inline">\(r\)</span> variables <span class="math inline">\(X_r\)</span> (unordered and correlated) <span class="math inline">\(\rightarrow\)</span> set of <span class="math inline">\(t\)</span> variables <span class="math inline">\(\xi_t\)</span>(ordered and uncorrelated), where <span class="math inline">\(t&lt;r\)</span></p>
<p><strong>First t principal components:</strong></p>
<p><span class="math display">\[
\xi_j = b_j^TX = b_{j1}X_1+...+b_{jr}X_r, j=1,2,...,t
\]</span></p>
<p>where we minimize the loss of information due to replacement.(<a href="#722-least-squares-optimality-of-pca">7.2.2</a>,<a href="#723-pca-as-a-variance-maximization-technique">7.2.3</a>)</p>
<p>We here interpreted &quot;information&quot; with the <strong>total variation</strong> of the original input variables:</p>
<p><span class="math display">\[
\sum_{j=1}^r var(X_j)=tr(\Sigma_{XX})
\]</span></p>
<p>From the <strong>spectral decomposition theorem</strong>, we can write</p>
<p><span class="math display">\[
\Sigma_{XX}=U\Lambda U^T, U^TU=I_r
\]</span></p>
<p>where the diagonal matrix <span class="math inline">\(\Lambda\)</span> has diagonal elements the eigenvalues <span class="math inline">\(\left\{\lambda_j\right\}\)</span> of <span class="math inline">\(\Sigma_{XX}\)</span>, and the columns of <span class="math inline">\(U\)</span> are the eigenvectors of <span class="math inline">\(\Sigma_{XX}\)</span>.</p>
<p>Thus, we obtain the total variation by</p>
<p><span class="math display">\[
tr(\Sigma_{XX})=tr(\Lambda)=\sum_{j=1}^r\lambda_j
\]</span></p>
<p>The <span class="math inline">\(j\)</span>th coefficient vector <span class="math inline">\(b_j=(b_{1j},...,b_{rj})^T\)</span> is chosen so that:</p>
<ul>
<li>The first <span class="math inline">\(t\)</span> linear projections <span class="math inline">\(\xi_j,j=1,2,...,t\)</span> of <span class="math inline">\(X\)</span> are ranked in importance through their variances <span class="math inline">\(\left\{var(\xi_j)\right\}\)</span> that <span class="math inline">\(var(\xi_1)\geq var(\xi_2)\geq ...\geq var(\xi_t)\)</span></li>
<li><span class="math inline">\(\xi_j\)</span> is uncorrelated with all <span class="math inline">\(\xi_k,k&lt;j\)</span></li>
</ul>
<p>There are two popular derivations of the set of principal components of <span class="math inline">\(X\)</span>:</p>
<ul>
<li>Least-Squares Optimality crieterion (analogous with regression)</li>
<li>Variance-maximizing technique</li>
</ul>
<h3 id="least-squares-optimality-of-pca">7.2.2 Least-Squares Optimality of PCA</h3>
<p>Let</p>
<p><span class="math display">\[
B=(b_1,...,b_t)^T
\]</span></p>
<p>be a <span class="math inline">\((t\times r)\)</span>-matrix of weights (<span class="math inline">\(t\leq r\)</span>), then the linear projections can be written as</p>
<p><span class="math display">\[
\xi = BX
\]</span></p>
<p>where <span class="math inline">\(\xi=(\xi_1,...,\xi_t)^T\in\mathbb{R}^{t\times n}\)</span>. Note that <span class="math inline">\(X\in\mathbb{R}^{r\times n}\)</span> is the observations matrix with variables as rows and individual observations as columns.</p>
<p>We want to find a <span class="math inline">\(r\)</span>-vector <span class="math inline">\(\mu\)</span> and an (<span class="math inline">\(r\times t\)</span>)-matrix <span class="math inline">\(A\)</span> such that</p>
<p><span class="math display">\[
X\approx \mu+A\xi
\]</span></p>
<p>(用 <span class="math inline">\(X\)</span> 对 <span class="math inline">\(\xi\)</span> 回归，类似“压缩”后“还原”的思想，通过“还原”后与原本数据的误差来评估“压缩器”的好坏)</p>
<p>Here we use least-squares error criterion to find the optimum <span class="math inline">\(\mu,A\)</span>:</p>
<p><span class="math display">\[
E[(X-\mu-A\xi)^T(X-\mu-A\xi)]
\]</span></p>
<p>recall that we have <span class="math inline">\(\xi=BX\)</span>, we can write</p>
<p><span class="math display">\[
E[(X-\mu-ABX)^T(X-\mu-ABX)]
\]</span></p>
<p>When <span class="math inline">\(t=1\)</span>, the least-squares problem is equivalent to</p>
<p><span class="math display">\[
\min_{\mu,A,B}E\sum_{j=1}^r(X_j-\mu_j-a_{j1}b_1^TX)^2
\]</span></p>
<p>where <span class="math inline">\(X=(X_1,...,X_r)^T,\mu=(\mu_1,...,\mu_r)^T,A=a_1=(a_{11},...,a_{r1})^T\)</span>, and <span class="math inline">\(B=b_1^T\)</span></p>
<p>For <span class="math inline">\(t&gt;1\)</span>, the criterion is similar to <a href="#appendix-a-weighted-sum-of-squares-criterion">Weighted Sum-of-Squares Criterion</a> with <span class="math inline">\(Y\equiv X,s=r,\Gamma=I_r\)</span>. Hence, the minimum solution is obtained by reduced-rank regression</p>
<p><span class="math display">\[
A^{(t)}=(v_1,...,v_t)=B^{(t)T},
\mu^{(t)}=(I_r-A^{(t)}B^{(t)})\mu_X
\]</span></p>
<p>where <span class="math inline">\(v_j=v_j(\Sigma_{XX})\)</span> is the eigenvector associated with the <span class="math inline">\(j\)</span>th largest eigenvalue of <span class="math inline">\(\Sigma_{XX}\)</span> (i.e.<span class="math inline">\(\lambda_j\)</span>).</p>
<p>Thus, our best rank-<span class="math inline">\(t\)</span> approximation to the original <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
\hat{X}^{(t)}=\mu^{(t)}+C^{(t)}X=\mu_X+C^{(t)}(X-\mu_X)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
C^{(t)}=A^{(t)}B^{(t)}=\sum_{j=1}^t v_jv_j^T
\]</span></p>
<p>is the reduced-rank regression coefficient matrix with rank <span class="math inline">\(t\)</span> for the principal components case.</p>
<p>The minumum of <span class="math inline">\(E[(X-\mu-ABX)^T(X-\mu-ABX)]\)</span> is given by <span class="math inline">\(\sum_{j=t+1}^r \lambda_j\)</span>.</p>
<p>The <em>first <span class="math inline">\(t\)</span> principal components</em> of <span class="math inline">\(X\)</span> are given by</p>
<p><span class="math display">\[
\xi_j=v_j^TX
\]</span></p>
<p>and the covariance between <span class="math inline">\(\xi_i\)</span> and <span class="math inline">\(\xi_j\)</span> is</p>
<p><span class="math display">\[
cov(\xi_i,\xi_j)=v_i^T\Sigma_{XX}v_j=v_i^T\lambda_jv_j=\delta_{ij}\lambda_j
\]</span></p>
<h3 id="pca-as-a-variance-maximization-technique">7.2.3 PCA as a Variance-Maximization Technique</h3>
<p>In the original derivation of principal components, the coefficient vectors</p>
<p><span class="math display">\[
b_j=(b_{j1},...,b_{jr})^T,j=1,2,...,t
\]</span></p>
<p>were chosen in a sequential manner so that:</p>
<ul>
<li><span class="math inline">\(var(\xi_j)=b_j^T\Sigma_{XX}b_j\)</span> are arranged in descending order subject to the normalizations <span class="math inline">\(b_j^Tb_j=1,j=1,...,t\)</span></li>
<li><span class="math inline">\(cov(\xi_i,\xi_j)=b_i^T\Sigma_{XX}b_j=0,i&lt;j\)</span></li>
</ul>
<p>The first principal component <span class="math inline">\(\xi_1\)</span> is obtained by choosing the <span class="math inline">\(r\)</span> coefficients <span class="math inline">\(b_1\)</span> for the linear projection <span class="math inline">\(\xi_1\)</span> (subject to normalization). A unique choice of <span class="math inline">\(\left\{\xi_j\right\}\)</span> is obtained through the normalization constraint <span class="math inline">\(b_j^Tb_j=1,j=1,...,t\)</span></p>
<p><span class="math display">\[
\begin{aligned}
    f(b_1)&amp;=var(\xi_1)+\lambda_1(1-b_1^Tb_1)\\
    &amp;=b_1^T\Sigma_{XX}b_1+\lambda_1(1-b_1^Tb_1)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\lambda_1\)</span> is a Lagrangian multiplier.</p>
<p><span class="math display">\[
\frac{\partial f(b_1)}{\partial b_1}=2(\Sigma_{XX}-\lambda_1I_r)b_1\overset{set}{=}0
\]</span></p>
<p>We require <span class="math inline">\(b_1\neq{0}\)</span>, so <span class="math inline">\(|\Sigma_{XX}-\lambda_1I_r|=0\)</span>.</p>
<p>Note that when <span class="math inline">\(\lambda_1\)</span> is the largest eigenvalue of <span class="math inline">\(\Sigma_{XX}\)</span>, let <span class="math inline">\(b_1\)</span> be the relevant eigenvector. We have <span class="math inline">\(\Sigma_{XX}b_1=\lambda_1b_1\)</span> hence <span class="math inline">\(|\Sigma_{XX}-\lambda_1I_r|=0\)</span>.</p>
<p><strong>Why has to be the largest?</strong></p>
<ul>
<li>If not, <span class="math inline">\(var(\xi_1)=b_1^T\Sigma_{XX}b_1=\lambda_1b_1^Tb_1=\lambda_1\)</span> is not the largest case.</li>
</ul>
<p>Similarly, the second principal component <span class="math inline">\(\xi_2\)</span> and coefficients <span class="math inline">\(b_2\)</span> are chosen. That is, maximize <span class="math inline">\(var(\xi_2)=b_2^T\Sigma_{XX}b_2\)</span> subject to <span class="math inline">\(b_2^Tb_2=1\)</span> and <span class="math inline">\(b_1^Tb_2=0\)</span></p>
<p><span class="math display">\[
f(b_2)=b_2^T\Sigma_{XX}b_2-\lambda_2(1-b_2^Tb_2)+\mu b_1^Tb_2
\]</span></p>
<p>where <span class="math inline">\(\lambda_2,\mu\)</span> are Lagrangian multipliers.</p>
<p><span class="math display">\[
\frac{\partial f(b_2)}{\partial b_2}=2(\Sigma_{XX}-\lambda_2I_r)b_2+\mu b_1\overset{set}{=}0
\]</span></p>
<p>Premultiplying <span class="math inline">\(b_1^T\)</span> then</p>
<p><span class="math display">\[
2b_1^T\Sigma_{XX}b_2+\mu=0
\]</span></p>
<p>Premultiplying <span class="math inline">\((\Sigma_{XX}-\lambda_1I_r)b_1=0\)</span> by <span class="math inline">\(b_2^T\)</span> yields <span class="math inline">\(b_2^T\Sigma_{XX}b_1=0\)</span>, whence <span class="math inline">\(\mu=0\)</span>.</p>
<p>This means that <span class="math inline">\(\lambda_2\)</span> is the second largest eigenvalue of <span class="math inline">\(\Sigma_{XX}\)</span> and <span class="math inline">\(b_2\)</span> is the relevant eigenvector.</p>
<p>In this sequential manner, we obtain the remaining sets of coefficients for the principal components.</p>
<h3 id="sample-principal-components">7.2.4 Sample Principal Components</h3>
<p>We estimate the principal components of <span class="math inline">\(X\)</span> using a random sample with observed values <span class="math inline">\(\left\{x_i\right\}\)</span></p>
<p><span class="math display">\[
\hat{\mu}_X=\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i
\]</span></p>
<p>Let <span class="math inline">\(x_{ci}=x_i-\bar{x}\)</span> and <span class="math inline">\(X_c=(x_{c1},...,x_{cn})\)</span> we estimate <span class="math inline">\(\Sigma_{XX}\)</span> by</p>
<p><span class="math display">\[
\hat{\Sigma}_{XX}=\frac{1}{n}X_cX_c^T
\]</span></p>
<p>The ordered eigenvalues of <span class="math inline">\(\hat{\Sigma}_{XX}\)</span> are denoted by <span class="math inline">\(\hat{\lambda}_1\geq\hat{\lambda}_2\geq...\geq\hat{\lambda}_r\geq 0\)</span> and the relevent eigenvector <span class="math inline">\(\hat{v}_j\)</span>.</p>
<p>Then we estimate <span class="math inline">\(A^{(t)}\)</span> and <span class="math inline">\(B^{(t)}\)</span> by</p>
<p><span class="math display">\[
\hat{A}^{(t)}=(\hat{v}_1,...,\hat{v}_t)=\hat{B}^{(t)T}
\]</span></p>
<p>The best rank-<span class="math inline">\(t\)</span> reconstruction of <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
\hat{x}^{(t)}=\bar{x}+\hat{C}^{(t)}(x-\bar{x})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{C}^{(t)}=\hat{A}^{(t)}\hat{B}^{(t)}=\sum_{j=1}^t\hat{v}_j\hat{v}_j^T
\]</span></p>
<p>is the reduced-rank regression coefficient matrix.</p>
<p><em>The <span class="math inline">\(j\)</span>th sample PC score</em> is given by</p>
<p><span class="math display">\[
\hat{\xi}_j=\hat{v}_j^TX_c=\hat{v}_j^T(X-\bar{X})
\]</span></p>
<h3 id="selection-of-the-number-of-components">7.2.5 Selection of the number of components</h3>
<p><strong>Scree Plot:</strong> The sample eigenvalues from a PCA are ordered from largest to smallest. (Find the &quot;elbow&quot;)</p>
<p><img src="/2020/05/18/20200518LDR/2020-05-17-05-31-45.png"></p>
<p><strong>PC Rank Trace:</strong> Obtain a useful estimate of the rank of the regression coefficient matrix <span class="math inline">\(C\)</span>.</p>
<p><span class="math display">\[
\Delta\hat{C}^{(t)}=(1-\frac{t}{r})^{1/2}
\]</span></p>
<p><span class="math display">\[
\Delta\hat{\Sigma}_{\epsilon\epsilon}^{(t)}=(\frac{\hat{\lambda}_{t+1}^2+...+\hat{\lambda}_r^2}{\hat{\lambda}_1^2+...+\hat{\lambda}_r^2})^{1/2}
\]</span></p>
<p>A plot of <span class="math inline">\(\Delta\hat{\Sigma}_{\epsilon\epsilon}^{(t)}\)</span> against <span class="math inline">\(\Delta\hat{C}^{(t)}\)</span> is the PC rank trace plot.(Find the &quot;elbow&quot;)</p>
<p><img src="/2020/05/18/20200518LDR/2020-05-17-05-36-49.png"></p>
<p><strong>Kaiser's Rule:</strong> Only those eigenvalues exceed 1 would be retained. (too controversial)</p>
<h3 id="invariance-and-scaling">7.2.6 Invariance and Scaling</h3>
<p>Note that the principal components are <strong>NOT invariant</strong> under rescalings of the initial variables. (sensitive to the units of measurement of the different input variables)</p>
<p>One possible way to solve this is to <strong>rescale</strong> the input data</p>
<p><span class="math display">\[
Z\leftarrow(diag\left\{\hat{\Sigma}_{XX}\right\})^{-1/2}(X-\hat{\mu}_X)
\]</span></p>
<p>which is equivalent to carrying out PCA using the <strong>correlation matrix</strong> (rather than the covariance matrix).</p>
<h3 id="what-can-be-gained-from-using-pca">7.2.7 What can be gained from using PCA</h3>
<ul>
<li>decorrelate the original variables</li>
<li>carry out data compression</li>
<li>reconstruct the original input data using a reduced number of variables according to a least-squares criterion</li>
<li>identify potential clusters in the data.</li>
</ul>
<p>PCA can be misleading when there are <strong>outliers</strong> in the data, or the <strong>linearity</strong> of PCA fits badly to the data.</p>
<h2 id="canonical-variate-and-correlation-analysis">7.3 Canonical Variate and Correlation Analysis</h2>
<h3 id="canonical-variates-and-canonical-correlations">7.3.1 Canonical Variates and Canonical Correlations</h3>
<p>Assume that</p>
<p><span class="math display">\[
\left(
\begin{matrix}
    X\\Y
\end{matrix}
\right)
\]</span></p>
<p>is a collection of <span class="math inline">\(r+s\)</span> variables partitioned into two disjoint subcollections, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are jointly distributed with mean vector and covariance matrix given by</p>
<p><span class="math display">\[
E\left\{\left(\begin{matrix}X\\Y\end{matrix}\right)\right\}=\left(\begin{matrix}\mu_X\\\mu_Y\end{matrix}\right)
\]</span></p>
<p><span class="math display">\[
E\left\{\left(\begin{matrix}X-\mu_X\\Y-\mu_Y\end{matrix}\right)\left(\begin{matrix}X-\mu_X\\Y-\mu_Y\end{matrix}\right)^T\right\}=\left(\begin{matrix}\Sigma_{XX}&amp;\Sigma_{XY}\\\Sigma_{YX}&amp;\Sigma_{YY}\end{matrix}\right)
\]</span></p>
<p>CVA seeks to replace the two sets of correlated variables by <span class="math inline">\(t\)</span> pairs of new variables</p>
<p><span class="math display">\[
(\xi_i,\omega_i),i=1,2,...,t,\quad t\leq\min(r,s)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{cases}
    \xi_j=g_j^TX=g_{1j}X_1+...+g_{rj}X_r\\
    \omega_j=h_j^TY=h_{1j}Y_1+...+h_{sj}Y_s
\end{cases}
\]</span></p>
<p>The <span class="math inline">\(j\)</span>th pairs of coefficient vectors are chosen so that</p>
<ul>
<li>the pairs are ranked in importance through their correlations that <span class="math inline">\(\rho_1\geq...\geq\rho_t\)</span> <span class="math display">\[
\rho_j=corr(\xi_j,\omega_j)=\frac{g_j^T\Sigma_{XY}h_j}{(g_j^T\Sigma_{XX}g_j)^{1/2}(h_j^T\Sigma_{YY}h_j)^{1/2}}
\]</span></li>
<li><span class="math inline">\(cov(\xi_j,\xi_k)=g_j^T\Sigma_{XX}g_k=0,k&lt;j.\)</span></li>
<li><span class="math inline">\(cov(\omega_j,\omega_k)=h_j^T\Sigma_{XX}h_k=0,k&lt;j.\)</span></li>
</ul>
<p>The paris are known as <em>the first <span class="math inline">\(t\)</span> pairs of canonical variates</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and their correlations as <em>the <span class="math inline">\(t\)</span> largest canonical correlations</em>.</p>
<h3 id="least-squares-optimality-of-cva">7.3.2 Least-Squares Optimality of CVA</h3>
<p>Let the <span class="math inline">\((t\times r)\)</span>-matrix <span class="math inline">\(G\)</span> and the <span class="math inline">\((t\times s\)</span>-matrix <span class="math inline">\(H\)</span>, with <span class="math inline">\(1\leq t\leq\min(r,s)\)</span> be such that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are linearly projected</p>
<p><span class="math display">\[
\xi=GX,\quad\omega=HY
\]</span></p>
<p>Consider the problem of finding <span class="math inline">\(\nu,G,H\)</span> so that</p>
<p><span class="math display">\[
HY\approx\nu+GX
\]</span></p>
<p>which minimize the <span class="math inline">\((t\times t)\)</span>-matrix</p>
<p><span class="math display">\[
E\left\{(HY-\nu-GX)(HY-\nu-GX)^T\right\}
\]</span></p>
<p>where we assume that the covariance matrix of <span class="math inline">\(\omega\)</span> is <span class="math inline">\(\Sigma_{\omega\omega}=H\Sigma_{YY}H^T=I_t\)</span></p>
<p>According to the Poincaré Separation Theorem, the cirterion is minimize when the coloumns of <span class="math inline">\(U\)</span> are the eigenvectors relevent to the first <span class="math inline">\(t\)</span> eigenvalues of <span class="math inline">\(R\)</span>. (see Section 3.2.10)</p>
<p>Some conditions when the inequalities become equalities:</p>
<p><span class="math display">\[
\nu=H\mu_Y-G\mu_X
\]</span></p>
<p><span class="math display">\[
G=\Sigma_{\omega X}\Sigma_{XX}^{-1}=H\Sigma_{YX}\Sigma_{XX}^{-1}
\]</span></p>
<p><span class="math display">\[
U^T=H\Sigma_{YY}^{1/2},\quad U^TU=I_t
\]</span></p>
<p><span class="math display">\[
R=\Sigma_{YY}^{-1/2}\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1/2}
\]</span></p>
<p><strong>To summarize:</strong></p>
<p><span class="math display">\[
\nu^{(t)}=H^{(t)}\mu_Y-G^{(t)}\mu_X
\]</span></p>
<p><span class="math display">\[
G^{(t)}=\left(\begin{matrix}v_1^T\\ \vdots \\ v_t^T\end{matrix}\right)\Sigma_{YY}^{-1/2}\Sigma_{YX}\Sigma_{XX}^{-1}=\left(\begin{matrix}\lambda_1u_1^T\\ \vdots \\ \lambda_tu_t^T\end{matrix}\right)\Sigma_{XX}^{-1/2}
\]</span></p>
<p><span class="math display">\[
H^{(t)}=\left(\begin{matrix}v_1^T\\ \vdots \\ v_t^T\end{matrix}\right)\Sigma_{YY}^{-1/2}
\]</span></p>
<p>where <span class="math inline">\(u_j\)</span> is the eigenvector relevent to the <span class="math inline">\(j\)</span>th largest eigenvalue <span class="math inline">\(\lambda_j^2\)</span> of</p>
<p><span class="math display">\[
R^*=\Sigma_{XX}^{-1/2}\Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}\Sigma_{XX}^{-1/2}
\]</span></p>
<p>and <span class="math inline">\(v_j\)</span> is the eigenvector relevent to the <span class="math inline">\(j\)</span>th largest eigenvalue <span class="math inline">\(\lambda_j^2\)</span> of</p>
<p><span class="math display">\[
R=\Sigma_{YY}^{-1/2}\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1/2}
\]</span></p>
<p>The <span class="math inline">\(jth\)</span> <em>pair of canonical variates socres</em> is given by</p>
<p><span class="math display">\[
\xi_j=g_j^TX,\quad\omega_j=h_j^TY
\]</span></p>
<p>where</p>
<p><span class="math display">\[
g_j=\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1/2}v_j=\lambda_j\Sigma_{XX}^{-1/2}u_j
\]</span></p>
<p><span class="math display">\[
h_j=\Sigma_{YY}^{-1/2}v_j
\]</span></p>
<h3 id="cva-as-a-correlation-maximization-technique">7.3.3 CVA as a Correlation-Maximization Technique</h3>
<p>Consider the linear poojections</p>
<p><span class="math display">\[
\xi=g^TX,\quad\omega=h^TY
\]</span></p>
<p>Assume that <span class="math inline">\(E(X)=\mu_X=0,E(Y)=\mu_Y=0\)</span> and <span class="math inline">\(g^T\Sigma_{XX}g=1,h^T\Sigma_{YY}h=1\)</span>.</p>
<p>We try to maximize the correlation</p>
<p><span class="math display">\[
corr(\xi,\omega)=g^T\Sigma_{XY}h
\]</span></p>
<p>The problem is equivalent to maximize</p>
<p><span class="math display">\[
f(g,h)=g^T\Sigma_{XY}h-\frac{1}{2}\lambda(g^T\Sigma_{XX}g-1)-\frac{1}{2}\mu(h^T\Sigma_{YY}h-1)
\]</span></p>
<p>where <span class="math inline">\(\lambda,\mu\)</span> are Lagrangian multipliers.</p>
<p><span class="math display">\[
\begin{cases}
\frac{\partial f}{\partial g}=\Sigma_{XY}h-\lambda\Sigma_{XX}g\overset{set}{=}0\\
\frac{\partial f}{\partial h}=\Sigma_{YX}g-\mu\Sigma_{YY}h\overset{set}{=}0
\end{cases}
\]</span></p>
<p>First, premultiplying <span class="math inline">\(g^T\)</span> and <span class="math inline">\(h^T\)</span> respectively, we have</p>
<p><span class="math display">\[
\begin{cases}
g^T\Sigma_{XY}h-\lambda g^T\Sigma_{XX}g=0\\
h^T\Sigma_{YX}g-\mu h^T\Sigma_{YY}h=0
\end{cases}
\]</span></p>
<p>whence, the correlation between <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\omega\)</span> satisfies</p>
<p><span class="math display">\[
g^T\Sigma_{XY}h=\lambda=\mu
\]</span></p>
<p>Hence, the original problem is equivalent to maximize the Lagrangrian multipliers.</p>
<p><strong>Eigen-decomposition method:</strong></p>
<p>Premultiplying <span class="math inline">\(\Sigma_{XX}^{-1}\)</span> and <span class="math inline">\(\Sigma_{YY}^{-1}\)</span> respectively</p>
<p><span class="math display">\[
\begin{cases}
    \Sigma_{XX}^{-1}\Sigma_{XY}h-\lambda g=0\\
    \Sigma_{YY}^{-1}\Sigma_{YX}g-\lambda h=0
\end{cases}
\]</span></p>
<p>Rearrange the two equalities</p>
<p><span class="math display">\[
\begin{cases}
    \Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}g=\lambda^2 g\\
    \Sigma_{YY}^{-1}\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}h=\lambda^2 h
\end{cases}
\]</span></p>
<p>It can be shown that <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are respectively chosen by the eigenvectors of</p>
<p><span class="math display">\[
\begin{cases}
    N=\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}\\
    N^*=\Sigma_{YY}^{-1}\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}
\end{cases}
\]</span></p>
<p>To maximize <span class="math inline">\(\lambda\)</span>, choose the eigenvectors relevant to the largest <span class="math inline">\(t\)</span> eigenvalues of <span class="math inline">\(N,N^*\)</span>.</p>
<p><strong>Least-square form:</strong></p>
<p>Premultiplying <span class="math inline">\(\Sigma_{YX}\Sigma_{XX}^{-1}\)</span> to <span class="math inline">\(\Sigma_{XY}h-\lambda\Sigma_{XX}g=0\)</span> we have</p>
<p><span class="math display">\[
\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}h=\lambda\Sigma_{YX}g
\]</span></p>
<p>Recall that <span class="math inline">\(\Sigma_{YX}g-\mu\Sigma_{YY}h=0\)</span>, we have</p>
<p><span class="math display">\[
\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}h=\lambda\mu\Sigma_{YY}h
\]</span></p>
<p><span class="math display">\[
\Leftrightarrow (\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}-\lambda^2\Sigma_{YY})h=0
\]</span></p>
<p>For there to be a nontrival solution, the following determinant has to be zero</p>
<p><span class="math display">\[
|\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}-\lambda^2\Sigma_{YY}|=0
\]</span></p>
<p><span class="math display">\[
\Leftrightarrow |\Sigma_{YY}^{-1/2}\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1/2}-\lambda^2I_s|=0
\]</span></p>
<p>It can be shown that the determinant above is a polynomial in <span class="math inline">\(\lambda^2\)</span> of degree <span class="math inline">\(s\)</span>, having <span class="math inline">\(s\)</span> real roots <span class="math inline">\(\lambda_1^2\geq\lambda_2^2\geq...\geq\lambda_s^2\geq 0\)</span>, which are the ordered eigenvalues of</p>
<p><span class="math display">\[
R=\Sigma_{YY}^{-1/2}\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1/2}
\]</span></p>
<p>with relevant eigenvectors <span class="math inline">\(v_1,...,v_s\)</span>.</p>
<p>The resultant choice of <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are given by</p>
<p><span class="math display">\[
g_1=\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1/2}v_1,\quad h_1=\Sigma_{YY}^{-1/2}v_1
\]</span></p>
<p>Then the pairs are</p>
<p><span class="math display">\[
\xi_1=g_1^TX,\quad\omega_1=h_1^TY
\]</span></p>
<p>and their correlation is</p>
<p><span class="math display">\[
corr(\xi_1,\omega_1)=g_1^T\Sigma_{XY}h_1=\lambda_1
\]</span></p>
<p>Then we choose the other projections in this sequential manner by adding constraint</p>
<p><span class="math display">\[
g^T\Sigma_{XX}g_1=h^T\Sigma_{YY}h_1=0
\]</span></p>
<p><span class="math display">\[
corr(\xi,\omega_1)=g^T\Sigma_{XY}h_1=\lambda_1g^T\Sigma_{XX}g_1=0
\]</span></p>
<p><span class="math display">\[
corr(\omega,\xi_1)=h^T\Sigma_{YX}g_1=\lambda_1h^T\Sigma_{YY}h_1=0
\]</span></p>
<h2 id="appendix-a-weighted-sum-of-squares-criterion">Appendix A: Weighted Sum-of-Squares Criterion</h2>
<p><code>#TODO</code></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Multivariate-Statistics/" rel="tag"># Multivariate Statistics</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/03/19/20200319SVM/" rel="next" title="Support Vectors Machine">
                  <i class="fa fa-chevron-left"></i> Support Vectors Machine
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-text">7.1 Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#principal-component-analysis"><span class="nav-text">7.2 Principal Component Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#population-principal-components"><span class="nav-text">7.2.1 Population Principal Components</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#least-squares-optimality-of-pca"><span class="nav-text">7.2.2 Least-Squares Optimality of PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pca-as-a-variance-maximization-technique"><span class="nav-text">7.2.3 PCA as a Variance-Maximization Technique</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sample-principal-components"><span class="nav-text">7.2.4 Sample Principal Components</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#selection-of-the-number-of-components"><span class="nav-text">7.2.5 Selection of the number of components</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#invariance-and-scaling"><span class="nav-text">7.2.6 Invariance and Scaling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-can-be-gained-from-using-pca"><span class="nav-text">7.2.7 What can be gained from using PCA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#canonical-variate-and-correlation-analysis"><span class="nav-text">7.3 Canonical Variate and Correlation Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#canonical-variates-and-canonical-correlations"><span class="nav-text">7.3.1 Canonical Variates and Canonical Correlations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#least-squares-optimality-of-cva"><span class="nav-text">7.3.2 Least-Squares Optimality of CVA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cva-as-a-correlation-maximization-technique"><span class="nav-text">7.3.3 CVA as a Correlation-Maximization Technique</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#appendix-a-weighted-sum-of-squares-criterion"><span class="nav-text">Appendix A: Weighted Sum-of-Squares Criterion</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Yukei Yim"
    src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Yukei Yim</p>
  <div class="site-description" itemprop="description">学数学本是逆天而行</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Yukei7" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;Yukei7" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yukei0509@gmail.com" title="E-Mail &amp;rarr; mailto:yukei0509@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yukei Yim</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.2
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
